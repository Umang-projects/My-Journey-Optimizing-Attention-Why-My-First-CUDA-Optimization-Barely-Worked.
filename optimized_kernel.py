# -*- coding: utf-8 -*-
"""Optimized_Kernel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/118tCIkRq8FjFQaanNsR7CwscG9g8AajR
"""

// ==================================================================================
// 2. Optimized Kernel (Shared Memory & Tiling)
// ==================================================================================

// Optimized matmul using shared memory to compute Scores = Q * K^T
__global__ void matmul_tiled_kernel(const float* q, const float* k, float* scores, int seq_len, int d_k) {
    __shared__ float q_tile[TILE_WIDTH][TILE_WIDTH];
    __shared__ float k_tile[TILE_WIDTH][TILE_WIDTH];

    int tx = threadIdx.x;
    int ty = threadIdx.y;
    int row = blockIdx.y * TILE_WIDTH + ty;
    int col = blockIdx.x * TILE_WIDTH + tx;

    float sum = 0.0f;

    // Loop over tiles along the d_k dimension
    for (int t = 0; t < (d_k + TILE_WIDTH - 1) / TILE_WIDTH; ++t) {
        // Load Q tile
        if (row < seq_len && (t * TILE_WIDTH + tx) < d_k) {
            q_tile[ty][tx] = q[row * d_k + (t * TILE_WIDTH + tx)];
        }
        else {
            q_tile[ty][tx] = 0.0f;
        }

        // Load K tile (transposed access)
        if (col < seq_len && (t * TILE_WIDTH + ty) < d_k) {
            k_tile[tx][ty] = k[col * d_k + (t * TILE_WIDTH + ty)];
        }
        else {
            k_tile[tx][ty] = 0.0f;
        }

        __syncthreads(); // Wait for all threads in the block to finish loading

        // Compute dot product from tiles in shared memory
        for (int i = 0; i < TILE_WIDTH; ++i) {
            sum += q_tile[ty][i] * k_tile[tx][i];
        }

        __syncthreads(); // Wait for all threads to finish computation before next tile
    }

    if (row < seq_len && col < seq_len) {
        scores[row * seq_len + col] = sum;
    }
}